/**
 * OpenAI-Compatible Provider Base Class
 * 
 * Most LLM providers (OpenAI, MiniMax, Ollama, etc.) use an OpenAI-compatible API.
 * This base class implements the common logic for these providers.
 */

import type {
  LLMProvider,
  ProviderConfig,
  InvokeParams,
  InvokeResult,
  Message,
  MessageContent,
  TextContent,
  ImageContent,
  FileContent,
  Tool,
  ToolChoice,
  ToolChoiceExplicit,
  ResponseFormat,
  OutputSchema,
  JsonSchema,
} from "../types";

// ============================================================================
// Message Normalization Helpers
// ============================================================================

const ensureArray = (
  value: MessageContent | MessageContent[]
): MessageContent[] => (Array.isArray(value) ? value : [value]);

const normalizeContentPart = (
  part: MessageContent
): TextContent | ImageContent | FileContent => {
  if (typeof part === "string") {
    return { type: "text", text: part };
  }

  if (part.type === "text") {
    return part;
  }

  if (part.type === "image_url") {
    return part;
  }

  if (part.type === "file_url") {
    return part;
  }

  throw new Error("Unsupported message content part");
};

const normalizeMessage = (message: Message) => {
  const { role, name, tool_call_id } = message;

  if (role === "tool" || role === "function") {
    const content = ensureArray(message.content)
      .map(part => (typeof part === "string" ? part : JSON.stringify(part)))
      .join("\n");

    return {
      role,
      name,
      tool_call_id,
      content,
    };
  }

  const contentParts = ensureArray(message.content).map(normalizeContentPart);

  // If there's only text content, collapse to a single string for compatibility
  if (contentParts.length === 1 && contentParts[0].type === "text") {
    return {
      role,
      name,
      content: contentParts[0].text,
    };
  }

  return {
    role,
    name,
    content: contentParts,
  };
};

const normalizeToolChoice = (
  toolChoice: ToolChoice | undefined,
  tools: Tool[] | undefined
): "none" | "auto" | ToolChoiceExplicit | undefined => {
  if (!toolChoice) return undefined;

  if (toolChoice === "none" || toolChoice === "auto") {
    return toolChoice;
  }

  if (toolChoice === "required") {
    if (!tools || tools.length === 0) {
      throw new Error(
        "tool_choice 'required' was provided but no tools were configured"
      );
    }

    if (tools.length > 1) {
      throw new Error(
        "tool_choice 'required' needs a single tool or specify the tool name explicitly"
      );
    }

    return {
      type: "function",
      function: { name: tools[0].function.name },
    };
  }

  if ("name" in toolChoice) {
    return {
      type: "function",
      function: { name: toolChoice.name },
    };
  }

  return toolChoice;
};

const normalizeResponseFormat = ({
  responseFormat,
  response_format,
  outputSchema,
  output_schema,
}: {
  responseFormat?: ResponseFormat;
  response_format?: ResponseFormat;
  outputSchema?: OutputSchema;
  output_schema?: OutputSchema;
}):
  | { type: "json_schema"; json_schema: JsonSchema }
  | { type: "text" }
  | { type: "json_object" }
  | undefined => {
  const explicitFormat = responseFormat || response_format;
  if (explicitFormat) {
    if (
      explicitFormat.type === "json_schema" &&
      !explicitFormat.json_schema?.schema
    ) {
      throw new Error(
        "responseFormat json_schema requires a defined schema object"
      );
    }
    return explicitFormat;
  }

  const schema = outputSchema || output_schema;
  if (!schema) return undefined;

  if (!schema.name || !schema.schema) {
    throw new Error("outputSchema requires both name and schema");
  }

  return {
    type: "json_schema",
    json_schema: {
      name: schema.name,
      schema: schema.schema,
      ...(typeof schema.strict === "boolean" ? { strict: schema.strict } : {}),
    },
  };
};

// ============================================================================
// OpenAI-Compatible Provider Base Class
// ============================================================================

export interface OpenAICompatibleConfig extends ProviderConfig {
  /** Default max tokens for completions */
  maxTokens?: number;
  /** Additional headers to send with requests */
  headers?: Record<string, string>;
}

/**
 * Base class for providers that use the OpenAI-compatible chat/completions API.
 * Extend this class for OpenAI, MiniMax, Ollama, and other compatible providers.
 */
export abstract class OpenAICompatibleProvider implements LLMProvider {
  abstract readonly name: string;
  
  protected config: OpenAICompatibleConfig;

  constructor(config: OpenAICompatibleConfig) {
    this.config = config;
  }

  /**
   * Get the full API URL for chat completions.
   * Override in subclasses if the endpoint path differs.
   */
  protected getApiUrl(): string {
    const baseUrl = this.config.baseUrl?.replace(/\/$/, "") || this.getDefaultBaseUrl();
    return `${baseUrl}/v1/chat/completions`;
  }

  /**
   * Get the default base URL for this provider.
   * Must be implemented by subclasses.
   */
  protected abstract getDefaultBaseUrl(): string;

  /**
   * Get the default model for this provider.
   * Must be implemented by subclasses.
   */
  protected abstract getDefaultModel(): string;

  /**
   * Check if the provider is properly configured.
   */
  isConfigured(): boolean {
    return Boolean(this.config.apiKey);
  }

  /**
   * Build request headers. Override in subclasses for custom auth schemes.
   */
  protected buildHeaders(): Record<string, string> {
    return {
      "content-type": "application/json",
      "authorization": `Bearer ${this.config.apiKey}`,
      ...this.config.headers,
    };
  }

  /**
   * Build the request payload. Override in subclasses for provider-specific options.
   */
  protected buildPayload(params: InvokeParams): Record<string, unknown> {
    const {
      messages,
      tools,
      toolChoice,
      tool_choice,
      outputSchema,
      output_schema,
      responseFormat,
      response_format,
      maxTokens,
      max_tokens,
      model,
    } = params;

    const payload: Record<string, unknown> = {
      model: model || this.config.model || this.getDefaultModel(),
      messages: messages.map(normalizeMessage),
    };

    if (tools && tools.length > 0) {
      payload.tools = tools;
    }

    const normalizedToolChoice = normalizeToolChoice(
      toolChoice || tool_choice,
      tools
    );
    if (normalizedToolChoice) {
      payload.tool_choice = normalizedToolChoice;
    }

    const resolvedMaxTokens = maxTokens || max_tokens || this.config.maxTokens;
    if (resolvedMaxTokens) {
      payload.max_tokens = resolvedMaxTokens;
    }

    const normalizedResponseFormat = normalizeResponseFormat({
      responseFormat,
      response_format,
      outputSchema,
      output_schema,
    });

    if (normalizedResponseFormat) {
      payload.response_format = normalizedResponseFormat;
    }

    return payload;
  }

  /**
   * Invoke the LLM API.
   */
  async invoke(params: InvokeParams): Promise<InvokeResult> {
    if (!this.isConfigured()) {
      throw new Error(`[${this.name}] Provider not configured - missing API key`);
    }

    const payload = this.buildPayload(params);

    const response = await fetch(this.getApiUrl(), {
      method: "POST",
      headers: this.buildHeaders(),
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(
        `[${this.name}] LLM invoke failed: ${response.status} ${response.statusText} â€“ ${errorText}`
      );
    }

    return (await response.json()) as InvokeResult;
  }

  /**
   * List available models. Default implementation throws not supported.
   * Override in subclasses that support this.
   */
  async listModels(): Promise<string[]> {
    throw new Error(`[${this.name}] listModels() not implemented`);
  }
}
